= Walking through the Spring Stack for Apache Kafka

This is the demo repository that goes along with the talk https://www.kafka-summit.org/sessions/walking-through-the-spring-stack-for-apache-kafka[Walking through the Spring Stack for Apache Kafka].

In order to run these demo applications, you need to have Kafka running on your `localhost:9092`.
If you have Kafka running elsewhere, please ensure that you update the properties for bootstrap server accordingly.

For convenience, we provide a docker-compose script, that allows you to run a three-node Kafka cluster locally and make one of them available at `localhost:9092`.

== Running Dockerized Kafka cluster locally.

* Checkout this repository
* Go to the root of this repository on a command line and do: `docker-compose up`.

== Demo applications

This repository contains several standalone demo applications.
Navigate to the README on each application for more details.

=== spring-kafka-app0 - Basic Application to Publish and Consume Text Based Records

This application creates a Kafka topic named `spring-kafka-app0-demo1` with a single partition.
Then it publishes random text data to this topic using `KafkaTemplate` and then consumes them using a `KafkaListener`.

`KafkaTemplate` and all its necessary components such as the `ProducerFactory` are auto-configured through Spring Boot.
Similarly, `KafkaListener` uses the standard `ConcurrentKafkaMessageListenerContainer` auto-configured through Spring Boot.

The application uses the https://github.com/DiUS/java-faker[Java Faker] library to generate 100 random Book recommendations with a one-second delay.
The listener simply consumes the records and prints them on the console.

=== spring-kafka-app1 - Advanced application with two publishers and listeners with initial assignment

In this application, we have two publishers both using custom `KafkaTemplate` beans.
One `KafkaTemplate` uses `Long` as key type and the other one uses `UUID`, while both of them use `String` as value type.
The publishers are initiated using REST endpoints.

App creates two Kafka topics - `spring-kafka-app1-demo1` and `spring-kafka-app1-demo2` both with 10 partitions.

Two listeners are provided using `KafkaListener`.
The first listener uses, `TopicPartition` and `PartitionOffset` annotations to control selective partitions and initial assignments on them.
The second listener is a variant of the first one, demonstrating how wildcard patterns can be used for partitions and initial assignments.
Take a look at the listeners for how they override the deserializer properties as opposed to what is auto-configured through Spring Boot.

In order to see the app in actions, run it (for e.g. on an IDE) and then issue the following two CURL commands.

```
curl -X POST  http://localhost:8080/publish/demo1
curl -X POST  http://localhost:8080/publish/demo2
```

The first command invokes the first publisher that publishes to topic `spring-kafka-app1-demo1` and the second one for the topic `spring-kafka-app1-demo2`.
Listeners log the data received on the console with the key, partition and offset it received from.

=== spring-kafka-app2 - JsonSerializer and JsonDeserializer in action

In this app, we use a POJO for publishing and consuming using the `JsonSerializer` and `JsonDeserializer` from Spring for Apache Kafka.

The application creates a Kafka topic - `spring-kafka-app2-demo`.
It publishes a domain object `Foo` into the topic and then consumes it using a `KafkaListener`.

Take a look at the `application.properties` configuration file for seeing how the serializer/deserializer properties are configured.

=== spring-kafka-app3 - Seeking to specific offsets

In this application, we have a publisher and a listener.
The listener extends from `AbstractConsumerSeekAware` which implements `ConsumerSeekAware`.
App creates a topic named `spring-kafka-app3-demo` with a single partition.

Each time the listener starts, we always seek the partition to offset 20.
The publisher sends 100 records when the app starts.

=== spring-kafka-app4 - Error Handling

This application demonstrates how container error handling work in Spring for Apache Kafka.
The application has a publisher which is invoked through a REST endpoint.
Each time the REST endpoint is called, the publisher sends 10 records to the topic `spring-kafka-app4-demo`.
On the consumer side, we have a contrived logic for throwing an exception for each time the offset is > 0 and divisible by 10.
By default, when the exception happens, the `DefaultErrorHandler` will kick in with 10 maximum tries without any backoff.
Now, uncomment the code in the `Listener` class
This code will create a custom `DefaultErrorHandler` which the boot autoconfiguration picks up and configure with the container factory.
The custom handler will re-try for a maximum of 3 times with a two seconds backoff each time.
The handler also is configured with a `DeadLetterPublishingRecoverer` which sends the record in error to a default DLT topic.
Next time, the app runs, you will notice that the first record (with offset % 10 ==0) tries to recover for 3 times and then being sent to the DLT.
There is also a convenient `KafkaListener` method for DLT where it logs the DLT messages.

In order to exercise the app, run it first and then issue the following CURL command.

```
curl -X POST  http://localhost:8080/publish
```

=== spring-kafka-app5 - ErrorHandlingDeserializer

This application demonstrates the `ErrorHandlingDeserializer` in action.
We have a publisher that sends data as `String`, but the consumer expects an `Integer`, thus it causes a deserialization error.
Take a look at the `application.properties` file to see how the `ErrorHandlingDeserializer` and the actual delegate types are used.
The application creates a topic named `spring-kafka-app5-demo`.
When the application starts, it sends a text to the topic, which the consumer receives and runs into a deserialization exception.
Since deserialization exceptions are not retried by default, the `DefaultErrorHandler` will not retry the records, but simply tries to recover.
We configure the error handler with a `DeadLetterPublishingRecoverer` for the records in error to be sent.
The application provides a convenient `KafkaListener` to consumer from the DLT topic (`spring-kafka-app5-demo.DLT`).

=== spring-kafka-app6 - Request-Reply using ReplyingKafkaTemplate

In this application we see how the request-reply semantics is made possible using the `ReplyingKafkaTemplate`.
There are two packages given - app6.request and app6.reply.
The applications rely on two Kafka topics - `kRequests` and `kReplies`.
Using the `ReplyingKafkaTemplate`, the request application sends a message to the topic `kRequests` and wait for the reply by blocking on the reply future.
The `ReplyingKafkaTemplate` is configured with a reply container that specifies the reply topic (`kReplies`) to wait on.
On the replies side, another application is provided that has a `KafkaListener` that listens on the topic where the requests are sent (`kRequests`).
Once the listener receives data, the listener converts the text to uppercase and then echo it back to the reply topic.
When doing so, it also adds the proper CORRELATION_ID it received as part of the request, so that the listen on the request side (in `ReplyingKafkaTemplate`) can properly discover it.

In order to run the app, do the following.

1. Start the reply app - `SpringKafkaApp6Reply`
2. Then start the request app - `SpringKafkaApp6Request`
3. Go to a terminal and issue the following command.

```
curl -X POST -H "Content-Type: text/plain" --data "hello request-reply in Spring for Apache Kafka" http://localhost:8080
```

On the console output on the request application, you should see the received text.



